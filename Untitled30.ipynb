{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DMadhumita2904/Research-Paper-Classification/blob/main/Untitled30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CxWKMb6m4j3",
        "outputId": "d2cd55e0-2243-411b-bf6a-ece6fd5077bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxjE8OEM2DZV",
        "outputId": "96bb236e-bf79-425d-c8b1-c8a5a5c7839e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9I3KVoO1j5j",
        "outputId": "93551418-e4a8-4527-9b85-b982c7ae2654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted text from: R006.pdf (Label: publishable)\n",
            "Extracted text from: R010.pdf (Label: publishable)\n",
            "Extracted text from: R011.pdf (Label: publishable)\n",
            "Extracted text from: R007.pdf (Label: publishable)\n",
            "Extracted text from: R008.pdf (Label: publishable)\n",
            "Extracted text from: R009.pdf (Label: publishable)\n",
            "Extracted text from: R012.pdf (Label: publishable)\n",
            "Extracted text from: R013.pdf (Label: publishable)\n",
            "Extracted text from: R014.pdf (Label: publishable)\n",
            "Extracted text from: R015.pdf (Label: publishable)\n",
            "Extracted text from: R001.pdf (Label: non-publishable)\n",
            "Extracted text from: R002.pdf (Label: non-publishable)\n",
            "Extracted text from: R003.pdf (Label: non-publishable)\n",
            "Extracted text from: R004.pdf (Label: non-publishable)\n",
            "Extracted text from: R005.pdf (Label: non-publishable)\n",
            "Extracted text from: np1.pdf (Label: non-publishable)\n",
            "Extracted text from: np2.pdf (Label: non-publishable)\n",
            "Extracted text from: np3.pdf (Label: non-publishable)\n",
            "Extracted text from: np4.pdf (Label: non-publishable)\n",
            "Extracted text from: np5.pdf (Label: non-publishable)\n",
            "Extracted text and labels saved to /content/drive/My Drive/labeled_papers_with_labels.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import json\n",
        "\n",
        "# Define folder paths (update these paths)\n",
        "publishable_folder = \"/content/drive/My Drive/Publishable\"\n",
        "non_publishable_folder = \"/content/drive/My Drive/Non_Publishable\"\n",
        "output_file = \"/content/drive/My Drive/labeled_papers_with_labels.json\"\n",
        "\n",
        "# Initialize list to store extracted text with labels\n",
        "labeled_data = []\n",
        "\n",
        "# Function to extract text from a folder and assign a label\n",
        "def extract_text_from_folder(folder_path, label):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            paper_id = filename.split(\".\")[0]  # Use filename (without extension) as the ID\n",
        "            pdf_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            try:\n",
        "                with pdfplumber.open(pdf_path) as pdf:\n",
        "                    # Extract text from all pages\n",
        "                    text = \"\"\n",
        "                    for page in pdf.pages:\n",
        "                        text += page.extract_text()\n",
        "\n",
        "                    # Append to the data list with label\n",
        "                    data.append({\"id\": paper_id, \"text\": text, \"label\": label})\n",
        "                    print(f\"Extracted text from: {filename} (Label: {label})\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "    return data\n",
        "\n",
        "# Extract text from publishable and non-publishable folders\n",
        "labeled_data.extend(extract_text_from_folder(publishable_folder, \"publishable\"))\n",
        "labeled_data.extend(extract_text_from_folder(non_publishable_folder, \"non-publishable\"))\n",
        "\n",
        "# Save the extracted text and labels to a JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, indent=4)\n",
        "\n",
        "print(f\"Extracted text and labels saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o97og7JY4EIT",
        "outputId": "c7b572f2-983b-4c05-e766-ebeb1dc1a1ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data saved to /content/drive/My Drive/preprocessed_papers.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (run only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load extracted JSON data\n",
        "input_file = \"/content/drive/My Drive/labeled_papers_with_labels.json\"\n",
        "output_file = \"/content/drive/My Drive/preprocessed_papers.json\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    labeled_data = json.load(f)\n",
        "\n",
        "# Initialize NLTK tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # Join words back into a single string\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Preprocess all papers\n",
        "preprocessed_data = []\n",
        "for paper in labeled_data:\n",
        "    processed_text = preprocess_text(paper[\"text\"])\n",
        "    if processed_text.strip():  # Ensure text is not empty\n",
        "        preprocessed_data.append({\n",
        "            \"id\": paper[\"id\"],\n",
        "            \"text\": processed_text,\n",
        "            \"label\": paper[\"label\"]\n",
        "        })\n",
        "    else:\n",
        "        print(f\"Paper {paper['id']} has no meaningful content after preprocessing.\")\n",
        "\n",
        "# Save preprocessed data\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(preprocessed_data, f, indent=4)\n",
        "\n",
        "print(f\"Preprocessed data saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQcJlJJY58ky",
        "outputId": "841621e4-9393-4a24-b9aa-e87c0c2efcb9"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/preprocessed_papers.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a3cb2d9c68a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load preprocessed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/preprocessed_papers.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlabeled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/preprocessed_papers.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load preprocessed data\n",
        "input_file = \"/content/drive/My Drive/preprocessed_papers.json\"\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    labeled_data = json.load(f)\n",
        "\n",
        "# Separate features and labels\n",
        "texts = [paper[\"text\"] for paper in labeled_data]\n",
        "labels = [1 if paper[\"label\"] == \"publishable\" else 0 for paper in labeled_data]  # 1: Publishable, 0: Non-Publishable\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 1: Feature Extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Step 2: Train a Classifier (Logistic Regression)\n",
        "classifier = LogisticRegression(random_state=42)\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 3: Evaluate the Model\n",
        "y_pred = classifier.predict(X_val_tfidf)\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Step 4: Save the Model and Vectorizer\n",
        "import joblib\n",
        "model_file = \"/content/drive/My Drive/research_paper_classifier.pkl\"\n",
        "vectorizer_file = \"/content/drive/My Drive/tfidf_vectorizer.pkl\"\n",
        "joblib.dump(classifier, model_file)\n",
        "joblib.dump(vectorizer, vectorizer_file)\n",
        "\n",
        "print(f\"Model saved to: {model_file}\")\n",
        "print(f\"Vectorizer saved to: {vectorizer_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbxq2fUl9OV7",
        "outputId": "32dc21ca-c52c-40d8-c722-31940fbc4147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Accuracy (mean across 5 folds): 0.95\n",
            "\n",
            "Classification Report (aggregated across all folds):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95        10\n",
            "           1       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Stratified K-Fold (preserves class distribution)\n",
        "k = 5\n",
        "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(classifier, vectorizer.transform(texts), labels, cv=kf, scoring='accuracy')\n",
        "predictions = cross_val_predict(classifier, vectorizer.transform(texts), labels, cv=kf)\n",
        "\n",
        "# Display cross-validation results\n",
        "print(f\"Cross-Validation Accuracy (mean across {k} folds): {np.mean(scores):.2f}\")\n",
        "print(\"\\nClassification Report (aggregated across all folds):\\n\")\n",
        "print(classification_report(labels, predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzNK47E1923T",
        "outputId": "aee52771-1d56-4a09-dbff-319b769f1dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.95\n",
            "Precision (non-publishable, publishable): [1.         0.90909091]\n",
            "Recall (non-publishable, publishable): [0.9 1. ]\n",
            "F1-Score (non-publishable, publishable): [0.94736842 0.95238095]\n",
            "Macro Average Precision: 0.95\n",
            "Macro Average Recall: 0.95\n",
            "Macro Average F1-Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# After obtaining predictions from cross-validation\n",
        "# predictions are obtained using cross_val_predict from the previous code\n",
        "# Now, calculate explicit metrics\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "# Precision (for each class)\n",
        "precision = precision_score(labels, predictions, average=None)  # average=None gives per-class precision\n",
        "\n",
        "# Recall (for each class)\n",
        "recall = recall_score(labels, predictions, average=None)  # average=None gives per-class recall\n",
        "\n",
        "# F1-Score (for each class)\n",
        "f1 = f1_score(labels, predictions, average=None)  # average=None gives per-class F1-Score\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision (non-publishable, publishable): {precision}\")\n",
        "print(f\"Recall (non-publishable, publishable): {recall}\")\n",
        "print(f\"F1-Score (non-publishable, publishable): {f1}\")\n",
        "\n",
        "# You can also calculate the macro average and weighted average metrics\n",
        "print(f\"Macro Average Precision: {precision.mean():.2f}\")\n",
        "print(f\"Macro Average Recall: {recall.mean():.2f}\")\n",
        "print(f\"Macro Average F1-Score: {f1.mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbBlikO4-1ZO",
        "outputId": "eecac8d9-5360-426c-cbdb-fb352e9b57d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "tCusw1GhAbyC",
        "outputId": "be3a645e-9c58-40fd-c3b9-7a18ed4b9590"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a7fc252-39dc-4e50-84af-4a9093ccfdb8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a7fc252-39dc-4e50-84af-4a9093ccfdb8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving P121.pdf to P121.pdf\n",
            "The paper is classified as 'Non-Publishable'.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "classifier = joblib.load(\"/content/drive/My Drive/research_paper_classifier.pkl\")\n",
        "vectorizer = joblib.load(\"/content/drive/My Drive/tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Use Google Colab's file upload feature\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file path\n",
        "for filename in uploaded.keys():\n",
        "    pdf_path = filename  # The uploaded file's name\n",
        "\n",
        "    # Extract text from the uploaded PDF\n",
        "    pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Convert the extracted text into TF-IDF features\n",
        "    pdf_features = vectorizer.transform([pdf_text])\n",
        "\n",
        "    # Predict the class\n",
        "    prediction = classifier.predict(pdf_features)\n",
        "\n",
        "    # Display the result\n",
        "    if prediction == 1:\n",
        "        print(\"The paper is classified as 'Publishable'.\")\n",
        "    else:\n",
        "        print(\"The paper is classified as 'Non-Publishable'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "TRGdkBz1Y99W",
        "outputId": "1e63a276-0e67-4939-9a29-e1db2e31252e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/KDSH_2025_Dataset/Papers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2b6c9298ad6e>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Process all papers in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only process PDF files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpaper_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/KDSH_2025_Dataset/Papers'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import joblib\n",
        "import PyPDF2\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "classifier = joblib.load(\"/content/drive/My Drive/research_paper_classifier.pkl\")\n",
        "vectorizer = joblib.load(\"/content/drive/My Drive/tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Folder containing the 135 papers (update with your actual folder path)\n",
        "papers_folder = \"/content/drive/My Drive/KDSH_2025_Dataset/Papers\"\n",
        "\n",
        "# Output CSV file path\n",
        "output_csv = \"/content/drive/My Drive/predictions.csv\"\n",
        "\n",
        "# Process all papers in the folder\n",
        "results = []\n",
        "for idx, file_name in enumerate(sorted(os.listdir(papers_folder))):\n",
        "    if file_name.endswith(\".pdf\"):  # Only process PDF files\n",
        "        paper_path = os.path.join(papers_folder, file_name)\n",
        "        paper_number = f\"P{idx + 1:03d}\"  # Format as P001, P002, etc.\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        pdf_text = extract_text_from_pdf(paper_path)\n",
        "\n",
        "        # Convert text into TF-IDF features\n",
        "        pdf_features = vectorizer.transform([pdf_text])\n",
        "\n",
        "        # Predict the class\n",
        "        prediction = classifier.predict(pdf_features)\n",
        "        classification = \"Publishable\" if prediction == 1 else \"Non-Publishable\"\n",
        "\n",
        "        # Append the result\n",
        "        results.append([paper_number, classification])\n",
        "\n",
        "# Save the results to a CSV file\n",
        "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Paper Number\", \"Classification\"])  # Write header\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Classification results saved to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OYlFY0cAoi8",
        "outputId": "49eb91d4-5fa9-4f66-f1b0-67d1d2048198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted all files to /content/extracted_pdfs\n",
            "Classification results saved to /content/drive/My Drive/predictions1.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import csv\n",
        "import joblib\n",
        "import PyPDF2\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the ZIP file in Google Drive\n",
        "zip_path = \"/content/drive/My Drive/testing.zip\"  # Replace with your ZIP file path\n",
        "extracted_folder = \"/content/extracted_pdfs\"  # Destination folder for extracted PDFs\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "print(f\"Extracted all files to {extracted_folder}\")\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "classifier = joblib.load(\"/content/drive/My Drive/research_paper_classifier.pkl\")\n",
        "vectorizer = joblib.load(\"/content/drive/My Drive/tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Output CSV file path\n",
        "output_csv = \"/content/drive/My Drive/predictions1.csv\"\n",
        "\n",
        "# Process all extracted PDFs\n",
        "results = []\n",
        "for idx, file_name in enumerate(sorted(os.listdir(extracted_folder))):\n",
        "    if file_name.endswith(\".pdf\"):  # Only process PDF files\n",
        "        paper_path = os.path.join(extracted_folder, file_name)\n",
        "        paper_number = f\"P{idx + 1:03d}\"  # Format as P001, P002, etc.\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        pdf_text = extract_text_from_pdf(paper_path)\n",
        "\n",
        "        # Convert text into TF-IDF features\n",
        "        pdf_features = vectorizer.transform([pdf_text])\n",
        "\n",
        "        # Predict the class\n",
        "        prediction = classifier.predict(pdf_features)\n",
        "        classification = \"Publishable\" if prediction == 1 else \"Non-Publishable\"\n",
        "\n",
        "        # Append the result\n",
        "        results.append([paper_number, classification])\n",
        "\n",
        "# Save the results to a CSV file\n",
        "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Paper Number\", \"Classification\"])  # Write header\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Classification results saved to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "FMVKEicCMgDs",
        "outputId": "7d53cec0-6ece-48b6-b9c8-a9814a01b1e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\x10'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7a1d83181a09>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0moutput_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"output.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mclassify_and_match_papers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results saved to {output_csv}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7a1d83181a09>\u001b[0m in \u001b[0;36mclassify_and_match_papers\u001b[0;34m(paper_dir, output_csv_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Main integration logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify_and_match_papers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mconference_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconference_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_conference_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7a1d83181a09>\u001b[0m in \u001b[0;36mload_model_and_vectorizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_and_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/tfidf_vectorizer.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvec_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/research_paper_classifier.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x10'."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import csv\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained classifier and vectorizer\n",
        "def load_model_and_vectorizer():\n",
        "    with open(\"/content/drive/My Drive/tfidf_vectorizer.pkl\", 'rb') as vec_file, open(\"/content/drive/My Drive/research_paper_classifier.pkl\", 'rb') as model_file:\n",
        "        vectorizer = pickle.load(vec_file)\n",
        "        classifier = pickle.load(model_file)\n",
        "    return vectorizer, classifier\n",
        "\n",
        "# Load conference embeddings and names\n",
        "def load_conference_data():\n",
        "    with open('conference_embeddings.npy', 'rb') as f:\n",
        "        conference_embeddings = np.load(f)\n",
        "    with open('conference_names.json', 'r') as f:\n",
        "        conference_names = json.load(f)\n",
        "    return conference_embeddings, conference_names\n",
        "\n",
        "# Extract and preprocess text from paper\n",
        "def extract_and_preprocess_text(paper_path):\n",
        "    # Example dummy text extraction and preprocessing\n",
        "    with open(paper_path, 'r') as file:\n",
        "        text = file.read().lower()\n",
        "    return text\n",
        "\n",
        "# Generate rationale for conference selection\n",
        "def generate_rationale(paper_text, conference_name):\n",
        "    return f\"This paper is suitable for {conference_name} because it aligns with the topics and trends typically covered by this conference.\"\n",
        "\n",
        "# Main integration logic\n",
        "def classify_and_match_papers(paper_dir, output_csv_path):\n",
        "    vectorizer, classifier = load_model_and_vectorizer()\n",
        "    conference_embeddings, conference_names = load_conference_data()\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Prepare results\n",
        "    results = []\n",
        "\n",
        "    for paper_id, paper_file in enumerate(sorted(os.listdir(paper_dir)), start=1):\n",
        "        paper_path = os.path.join(paper_dir, paper_file)\n",
        "        paper_id_str = f\"P{paper_id:03}\"\n",
        "        paper_text = extract_and_preprocess_text(paper_path)\n",
        "\n",
        "        # Step 1: Publishability Classification\n",
        "        tfidf_features = vectorizer.transform([paper_text])\n",
        "        publishable = classifier.predict(tfidf_features)[0]\n",
        "\n",
        "        if publishable == 1:\n",
        "            # Step 2: Conference Matching\n",
        "            paper_embedding = embedding_model.encode([paper_text])[0]\n",
        "            similarities = cosine_similarity([paper_embedding], conference_embeddings)[0]\n",
        "            best_conference_index = np.argmax(similarities)\n",
        "            best_conference_name = conference_names[best_conference_index]\n",
        "            rationale = generate_rationale(paper_text, best_conference_name)\n",
        "        else:\n",
        "            best_conference_name = \"NA\"\n",
        "            rationale = \"NA\"\n",
        "\n",
        "        # Append results\n",
        "        results.append({\n",
        "            \"Paper ID\": paper_id_str,\n",
        "            \"Publishable\": publishable,\n",
        "            \"Conference\": best_conference_name,\n",
        "            \"Rationale\": rationale\n",
        "        })\n",
        "\n",
        "    # Save to CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# Directory containing research papers and output file path\n",
        "paper_directory = \"path_to_papers\"\n",
        "output_csv = \"output.csv\"\n",
        "\n",
        "classify_and_match_papers(paper_directory, output_csv)\n",
        "print(f\"Results saved to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN2ero09i8jV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from sklearn.externals import joblib\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "# Step 1: Initialize Models and Utilities\n",
        "# Load pre-trained classifier and vectorizer for publishability check\n",
        "classifier = joblib.load(\"/content/drive/My Drive/research_paper_classifier.pkl\")\n",
        "vectorizer = joblib.load(\"/content/drive/My Drive/tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Initialize the Embedding Model and Text Generation Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "embedding_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "text_generation_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Define Conference Keywords and Reference Papers\n",
        "conference_keywords = {\n",
        "    \"CVPR\": [\"object detection\", \"image segmentation\", \"computer vision tasks\", \"convolutional networks\"],\n",
        "    \"EMNLP\": [\"language models\", \"semantic parsing\", \"text classification\", \"token embeddings\"],\n",
        "    \"KDD\": [\"data clustering\", \"knowledge discovery\", \"graph mining\", \"recommendation systems\"],\n",
        "    \"NeurIPS\": [\"stochastic gradient descent\", \"adversarial training\", \"multi-agent systems\", \"gradient stability\"],\n",
        "    \"TMLR\": [\"optimization techniques\", \"mathematical proofs\", \"theoretical guarantees\", \"learning rates\"]\n",
        "}\n",
        "\n",
        "# Reference papers for each conference (provided by the user)\n",
        "conference_papers = {\n",
        "    \"CVPR\": [\"/content/R006.pdf\", \"/content/R007.pdf\", \"/content/cvpr7.pdf\", \"/content/cvpr6.pdf\", \"/content/cvpr5.pdf\"],\n",
        "    \"EMNLP\": [\"/content/R008.pdf\", \"/content/R009.pdf\", \"/content/emnlp5.pdf\", \"/content/emnlp6.pdf\", \"/content/emnlp7.pdf\"],\n",
        "    \"KDD\": [\"/content/R010.pdf\", \"/content/R011.pdf\", \"/content/kdd6.pdf\", \"/content/kdd7.pdf\", \"/content/kdd5.pdf\"],\n",
        "    \"NeurIPS\": [\"/content/R012.pdf\", \"/content/R013.pdf\", \"/content/neurlps7.pdf\", \"/content/neurlps5.pdf\", \"/content/neurlps6.pdf\"],\n",
        "    \"TMLR\": [\"/content/R014.pdf\", \"/content/R015.pdf\", \"/content/tmlr7.pdf\", \"/content/tmlr5.pdf\", \"/content/tmlr6.pdf\"]\n",
        "}\n",
        "\n",
        "# Custom VectorStore Implementation with FAISS\n",
        "class SimpleVectorStore:\n",
        "    def __init__(self, dimension=768):\n",
        "        self.vectors = []\n",
        "        self.metadata = []\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    def add_vector(self, key, vector, metadata=None):\n",
        "        if len(vector.shape) == 1:\n",
        "            vector = vector.reshape(1, -1)\n",
        "        self.vectors.append({\"key\": key, \"vector\": vector, \"metadata\": metadata})\n",
        "        self.index.add(vector.astype(np.float32))\n",
        "\n",
        "    def search(self, query_vector=None, key=None, top_k=10):\n",
        "        if key:\n",
        "            return [v for v in self.vectors if v[\"key\"] == key]\n",
        "        elif query_vector is not None:\n",
        "            query_vector = query_vector.reshape(1, -1) if query_vector.ndim == 1 else query_vector\n",
        "            distances, indices = self.index.search(query_vector.astype(np.float32), top_k)\n",
        "            results = [\n",
        "                {\"key\": self.vectors[idx][\"key\"], \"score\": 1 / (1 + distances[0][i]), \"metadata\": self.vectors[idx][\"metadata\"]}\n",
        "                for i, idx in enumerate(indices[0])\n",
        "            ]\n",
        "            return results\n",
        "        return []\n",
        "\n",
        "vector_store = SimpleVectorStore()\n",
        "\n",
        "# Step 2: Function to Extract Text from PDFs\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and preprocesses the text extracted from a PDF.\"\"\"\n",
        "    text = \" \".join(text.split())  # Remove multiple spaces and newlines\n",
        "    text = re.sub(r\"(?i)References.*\", \"\", text)  # Remove references section\n",
        "    text = re.sub(r\"(Figure|Table) \\d+.*\", \"\", text)  # Remove figure/table captions\n",
        "    return text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts and preprocesses text from a given PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return preprocess_text(text.strip())\n",
        "\n",
        "# Step 3: Function to Generate Embeddings\n",
        "def create_embedding(text):\n",
        "    \"\"\"Generate embedding for the given text, handling chunking and truncation.\"\"\"\n",
        "    chunks = text.split()  # Split the text into words (no chunking needed for this case)\n",
        "    tokenized = tokenizer(\" \".join(chunks), truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        embedding = embedding_model(**tokenized).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Step 4: Conference Matching Function\n",
        "def compute_keyword_overlap(text, conference):\n",
        "    keywords = conference_keywords[conference]\n",
        "    overlap = sum(1 for word in keywords if word in text.lower())\n",
        "    return overlap\n",
        "\n",
        "def match_to_conference(new_pdf_path):\n",
        "    new_text = extract_text_from_pdf(new_pdf_path)\n",
        "    new_vector = create_embedding(new_text)\n",
        "\n",
        "    results = vector_store.search(query_vector=new_vector, top_k=10)\n",
        "\n",
        "    similarity_sums = defaultdict(float)\n",
        "    for result in results:\n",
        "        conference = result['key']\n",
        "        similarity = result['score']\n",
        "        similarity_sums[conference] += similarity\n",
        "\n",
        "    for conference in similarity_sums:\n",
        "        overlap_score = compute_keyword_overlap(new_text, conference)\n",
        "        similarity_sums[conference] += 0.5 * overlap_score\n",
        "\n",
        "    sorted_conferences = sorted(similarity_sums.items(), key=lambda x: x[1], reverse=True)\n",
        "    best_conference, best_score = sorted_conferences[0]\n",
        "\n",
        "    rationale = generate_rationale(best_conference, new_text)\n",
        "    return best_conference, best_score, rationale\n",
        "\n",
        "def generate_rationale(conference, new_paper_text):\n",
        "    conference_papers_list = []\n",
        "    for paper in conference_papers[conference]:\n",
        "        conference_papers_list.append(extract_text_from_pdf(paper))\n",
        "\n",
        "    conference_text = \" \".join(conference_papers_list)\n",
        "    input_text = f\"The new paper's content is as follows: {new_paper_text[:500]}... The conference {conference} focuses on {conference_text[:500]}...\"\n",
        "    rationale = text_generation_model(input_text, max_new_tokens=200, num_return_sequences=1)\n",
        "    return rationale[0]['generated_text']\n",
        "\n",
        "# Step 5: Function for Paper Classification and Conference Matching\n",
        "def classify_and_match_papers(input_folder):\n",
        "    rows = []\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            paper_id = filename.split(\".\")[0]\n",
        "            pdf_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            # Extract text and predict publishability\n",
        "            pdf_text = extract_text_from_pdf(pdf_path)\n",
        "            pdf_features = vectorizer.transform([pdf_text])\n",
        "            prediction = classifier.predict(pdf_features)\n",
        "\n",
        "            if prediction == 1:\n",
        "                # Publishable paper: match to a conference and generate rationale\n",
        "                conference, score, rationale = match_to_conference(pdf_path)\n",
        "                rows.append([paper_id, 1, conference, rationale[:100]])\n",
        "            else:\n",
        "                # Non-publishable paper: label as 'NA'\n",
        "                rows.append([paper_id, 0, \"NA\", \"NA\"])\n",
        "\n",
        "    # Step 6: Save Results to CSV\n",
        "    with open(\"/content/drive/My Drive/research_paper_classification.csv\", mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Paper ID\", \"Publishable\", \"Conference\", \"Rationale\"])\n",
        "        writer.writerows(rows)\n",
        "\n",
        "# Step 7: Run the Process\n",
        "input_folder = \"/content/drive/My Drive/Research_Papers\"\n",
        "classify_and_match_papers(input_folder)\n",
        "\n",
        "print(\"CSV file with results has been saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLknREJXfcB3k6pyawJZQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}